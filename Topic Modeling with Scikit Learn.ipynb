{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Topic Modeling with Scikit Learn\"\n",
    "# https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730\n",
    "# http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "# https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx)) \n",
    "        print( \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "#twitterDf = pd.read_csv('C:/Apps/Anaconda3/Python_workGH/Twitter Archive/tweets.csv') # my twitter archive downloaded\n",
    "#documents = list(twitterDf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "excl_words = set(['amp','https','rt','fom'])\n",
    "excludeWords =stop.union(excl_words)\n",
    "excludeChars = set(string.punctuation).union(set(['‘','’'])).difference(set(['@'])) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    #stop_free = \" \".join(word for word in doc.lower().split() if ((word not in excludeWords)&(word[0]!='@')) )\n",
    "    #punc_free = ''.join(ch for ch in stop_free if ch not in excludeChars)\n",
    "    #normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    \n",
    "    punc_free = ''.join(ch for ch in doc.lower() if ch not in excludeChars)\n",
    "    stop_free = \" \".join(word for word in punc_free.split() if (word not in excludeWords)&(word[0]!='@')&('htt' not in word))\n",
    "    #normalized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "    \n",
    "    return stop_free # normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [clean(doc) for doc in documents]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'well im sure story nad seem biased disagree statement us media ruin israels reputation rediculous us media proisraeli media world lived europe realize incidences one described letter occured us media whole seem try ignore us subsidizing israels existance europeans least degree think might reason report clearly atrocities shame austria daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt go away look jews treating races got power unfortunate'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of documents: \"+str(len(documents)))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "dont people think like know im good time right make\n",
      "Topic 1:\n",
      "windows file use files program using window card dos problem\n",
      "Topic 2:\n",
      "god jesus bible believe christ faith christian christians gods church\n",
      "Topic 3:\n",
      "thanks email advance know looking im hi info address information\n",
      "Topic 4:\n",
      "drive scsi drives hard disk controller ide floppy mac card\n",
      "Topic 5:\n",
      "key chip encryption clipper keys government use algorithm phone chips\n",
      "Topic 6:\n",
      "game team games year players season play hockey win teams\n",
      "Topic 0:\n",
      "game team 10 year games 25 play 12 15 season\n",
      "Topic 1:\n",
      "god people jesus believe true say bible jews evidence israel\n",
      "Topic 2:\n",
      "new car years armenian said armenians turkish time people went\n",
      "Topic 3:\n",
      "maxaxaxaxaxaxaxaxaxaxaxaxaxaxax key government encryption public use chip security law keys\n",
      "Topic 4:\n",
      "file available program information space use data email list db\n",
      "Topic 5:\n",
      "dont people think like know im right good say time\n",
      "Topic 6:\n",
      "use windows thanks drive like know problem using card im\n"
     ]
    }
   ],
   "source": [
    "no_topics = 7\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
